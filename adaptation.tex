\chapter{Online Adaptation}
\label{chap:adaptation}
%\url{https://github.com/clab/fast_align}
In this chapter, we experiment we online adaptation of the proposed enhanced ASR pipeline.

We assume that a speaker does many errors systematically, for example due his/her native language. We expect, some of these errors can be identified and corrected in the ASR phoneme-to-grapheme translation model. We plan to identify this corrections and extract simple rewriting rules. These rewriting rules would be applied prior to feeding them into the ASR/SLT translation model in the pipeline.

\XXX{chapter organization}

\section{Enhanced ASR/SLT Pipeline with Online Adaptation}
\label{oeasr:model}
We propose the enhanced ASR pipeline with online adaptation as follows: the acoustic model outputs the phoneme transcripts. The phoneme transcripts are adapted using the Online Adaptation model. The adapted output (still in phonemes) is fed into the phoneme-to-grapheme translation model. The translation model translate the phoneme input into graphemes using beam search. The model outputs the best translation candidate from the beam as translation of the whole ASR pipeline. All beam translations are passed through the \texttt{phonemizer} and the online adaptation model learns new rules from the transcripts.

In the case of SLT pipeline, two phoneme-to-grapheme translation models are needed: the ASR and the SLT. The ASR translation model would be used for obtaining ``traning data'' for the online adaptation model.

The schema of the proposed enhanced ASR pipeline with online adaptation is in \cref{fig:online_easr}.

\begin{figure}[t]
	\centering
	\includegraphics[width=.9\textwidth]{img/online_easr}
	\caption{Enhanced ASR pipeline with online adaptation.}
	\label{fig:online_easr}
\end{figure} 


\subsection{Online Adaptation Model}
The obvious requirement on the online adaptation model is that it must be able to learn quickly. Hence, we rule out neural networks as they require higher volume of training data. If not provided, the neural networks tend to over-fit the examples. 

Therefore, we decided for rule-based model. We take our inspiration in work we previously reviewed for phoneme-to-grapheme models (see \cref{easr:rel_asr}). More precisely, the work of \perscite{horndasch2006phoneme}. Their main objective is to find appropriate orthographic representations for phoneme strings. Our use case differs as we seek phoneme-to-phoneme mapping correcting errors instead of phoneme-to-grapheme mapping. Using a expectation-maximization algorithm \parcite{dempster1977maximum}, we seek the best ``correct-to-incorrect'' phoneme alignment (in their setup, they are looking for best phoneme-to-grapheme alignment). In the second step, we cluster neighboring symbols together to account for the insertions (i.e., to remove ``$\epsilon$ to phoneme'' rules). Finally, $n$-gram probabilities of symbol pairs are learned. During the inference, the input string is split into individual symbols. For each symbol are generated all possible symbol pairs. According to the beam width, the best sequences are taken. 

\paragraph{Model Training}
As already described, we learn the phoneme rewrite rules from aligned ``incorrect-to-correct'' phoneme transcriptions. To align these transcriptions, we utilize expectation-maximization algorithm as follows:

\begin{enumerate}
	\item Compute the initial \texttt{alignment} using modified Levenstein distance algorithm (insertion, deletion and substitution operations are considered, each with the same cost).
	
	\item \emph{While} substitution frequencies change do:
	\begin{enumerate}
		\litem{Expectation step:} based on the \texttt{alignment}, compute the substitution frequencies,
		
		\litem{Maximization step:} align training pairs  based on frequencies using dynamic programming.
	\end{enumerate}
\end{enumerate}

The last computed alignment defines the rewrite rules: ``rewrite the symbol from the source string (the ``incorrect'' one) to the symbol on the corresponding position in the target string (the ``correct'' one)''. The insertions in the alignment are represented with special $\epsilon$ character in the source string:

\begin{center}
	\begin{tabular}{c}
		\textipa{Ins3:}\large{$\epsilon\epsilon\epsilon$}  \\
		\textipa{Ins3:}\large{\textipa{S@n}}
	\end{tabular}
\end{center}

 We do not allow rules ``$\epsilon \rightarrow \dots$'', because they do not have straightforward representation during the beam search decoding. Instead, we cluster the rules with $\epsilon$ symbol as head with neighbors using following algorithm:
   
\begin{enumerate}
	\item For each \texttt{rule} in \texttt{alignment}:
	\begin{enumerate}
		\item If \texttt{rule} is \emph{empty}: join with previous or next; \texttt{rule}
		\item Else: add to \texttt{new\_rules}.
	\end{enumerate}
\end{enumerate}

From the above insertion example, we would extract following rewrite rules:

\begin{center}
	\begin{tabular}{c|c|c|c}
		\textipa{I} $\rightarrow$ \textipa{I} &
		\textipa{n} $\rightarrow$ \textipa{n} &
		\textipa{s} $\rightarrow$ \textipa{s} &
		\textipa{3:} $\rightarrow$ \textipa{3:S@n}.
	\end{tabular}
\end{center}

Finally, we train an $n$-gram model. The function of the $n$-gram model is to learn interactions between the neighboring phoneme rewriting rules. We utilize the KenLM for this task. As the KenLM is word-oriented, we dump the rules as separate words in form ``source-target''. We substitute the space symbol with underscore.

\paragraph{Inference}
We use the trained $n$-gram model during the inference --- rewriting input using the rules. For this, we utilize beam search:

\begin{enumerate}
	\item For each symbol from the input:
		\begin{enumerate}
			\item generate candidates using rules with corresponding source symbol,
			\item score the candidates using the $n$-gram model,
			\item keep top $w$ candidates according to the beam width $w$. 
		\end{enumerate}
\end{enumerate}



\section{Experiments}
\label{oeasr:experiments}
In this section, we experiment with different variations of the proposed online adaptation algorithm.

\XXX{For testing of the adaptation model, we use our own parallel Czech - English ASR/SLT test set \texttt{read-newstest} (see \cref{read-newstest}). To develop the adaptation model independently on the phoneme-to-grapheme model, we use instead of corrected transcripts that would be given by the P2G model the golden transcripts.}

\subsection{Na\"ive Approach: Train \& Rewrite All}
We first experiment with a rather ``na\"ive'' approach --- we use all sequences for training of the adaptation model and we rewrite all phoneme transcripts flowing through the model (i.e., disregarding correctness of the substrings/words).

We assume that the speaker makes errors systematically (e.g., instead of [\textipa{3:}] pronounces [\textipa{er}] in every word).

