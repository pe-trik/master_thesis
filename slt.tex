\chapter{Spoken Language Translation}
\label{chap:slt}
In the preceding chapter, we introduced and trained an ``Enhanced ASR''. The pipeline of the Enhanced ASR system consists of an acoustic model and a translation model. In the case of the ASR, the translation model rewrites phonetic transcriptions to graphemes in the language of the input. 

In this chapter, we develop a Spoken Language Translation pipeline for Czech and English language --- in both directions. We build the pipeline architecture upon previously proposed Enhanced ASR. From the Enhanced ASR, we take the acoustic model, which remains the identical. The essential difference is the change of target language for the translation model. 

We organize this chapter as follows: first, we analyze associated work on SLT in \cref{slt:related}. Next follows our motivation and experiment outline in \cref{slt:outline}. In \cref{slt:training} we describe training process and in \cref{slt:evaluation} we evaluate the trained systems. Finally, we conclude the experiment in \cref{slt:conclusion}.

\section{Related work}
\label{slt:related}

With the advent of Deep Learning, we observe a clear trend towards End-To-End models in SLT.

For instance, \perscite{weiss2017sequence} presents a attention-based model. They use the very architecture as ASR --- encoder, and decoder. Their End-to-End trained network outperforms ASR-MT cascade on Spanish-English speech translation task.

Our great inspiration in our work and particularly for our Spoken Language Translation, is the study of \perscite{salesky-etal-2019-exploring}. They examine an alternative speech feature representation for the End-To-End Speech Translation. The authors propose to use compressed phoneme-like speech representation. Their method first generates phoneme labels for a speech utterance. Second, consecutive frames with the same label are averaged-out. To compute the phoneme labels, authors use a hybrid HMM-DNN system implemented in Kaldi \parcite{Kaldi}. For the translation, they use a sequence-to-sequence model based on LSTM. The authors observe an improvement of the translation quality up to 5 BLEU on low- and high-resource language pairs.


\section{Motivation and experiment outline}
\label{slt:outline}
We propose to split the SLT pipeline to the acoustic and SLT model. As an intermediate representation, we implement phonemes. We are highly motivated by their success in the \perscite{salesky-etal-2019-exploring}. Also, as reviewed in earlier chapters on ASR (\cref{chapter:asr,chap:enhanced_asr}), phonemes are a competitive representation unit in speech processing.  

In our Spoken Language Translation pipeline, we take the acoustic model from the \cref{chapter:asr}. Specifically, we use the ASR to the phonemes. As a translation model, we utilize the Transformer architecture.

We compare our proposed SLT system with a more conventional SLT pipeline --- an ASR-MT tandem.

\section{Training}
\label{slt:training}
Following the experiment outline from the preceding section, we train two translation models. Additionally, we also train baseline translation models. 

The training procedure remains mostly identical to the training scheme of the Enhanced ASR (see \cref{easr:training}). As we concluded in \cref{easr:conclusion}, the bigger Transformer is a better fit for the ASR task. Furthermore, the translation task to a different language is an even more difficult problem. Accordingly, we work only with the Transformer ``big'' in this chapter.

For evaluation during the training of baseline models, we decided for \texttt{news\-test\-2015}. For our phoneme-to-grapheme models, we phonemize the source side of the \texttt{news\-test\-2015} using \texttt{phonemizer}.

Again, as determined in \cref{easr:tok_conclusion}, we use separate vocabularies for source and target languages. Correspondingly, we use 32k vocabulary size. Both BPEs are trained on a clean, phonemized Czeng dataset.

For training, we utilize following hyper-parameters for Transformer ``big'':
\begin{itemize}
	\item GPU(s): 10 GPUs with at least 11 GB VRAM
	\item batch size: 2000
	\item learning rate: 0.03
	\item warm-up steps: 8000
	\item max steps: 600000 with manual abortion
\end{itemize}

To attain better regularization, we involve the BPE drop-out technique proposed by \parcite{provilkov2019bpe}. We appoint the drop-out probability to 0.1, following the best setting from the original paper. The drop-out is applied to both the source and target.

The progression of training as evaluations on \texttt{newstest2015} is depicted for Czech to English in the \cref{fig:cs_en} and for English to Czech in the \cref{fig:en_cs}. We can notice a gap between 

\begin{figure*}[h]
	\includegraphics[width=\linewidth,height=7cm]{img/cs_en}
	\caption{Evaluations on newstest2015 during the training. Phonemized Czech side as source and (original, in graphemes) English side as target. Baseline model is tested on original source.}
	\label{fig:cs_en}
\end{figure*}

\begin{figure*}[h]
	\includegraphics[width=\linewidth,height=7cm]{img/en_cs}
	\caption{Evaluations on newstest2015 during the training. Phonemized English side as source and (original, in graphemes) Czech side as target. Baseline model is tested on original source.}
	\label{fig:en_cs}
\end{figure*}

\section{Evaluation}
\label{slt:evaluation}
In this section we evaluate trained models.

We evaluate an SLT pipeline consisting of an ASR and an NMT model. More precisely, our proposed pipeline includes an acoustic model --- an ASR to phonemes --- and a translation model --- an NMT from phonemes to graphemes in the target language. 

Note, all models used in this comparison are trained on the same data sets. The only contrast is the phonemization of transcripts for our acoustic model and translation source for the SLT translation model.

Again, as in the previous chapter, we apply different beam sizes during the evaluation.

\paragraph{Beam size} 
SacreBLEU scores in the \cref{tab:eval_slt,tab:eval_slt_en_cs} are consistent with the established in the Neural Machine Translation. We see that the models are, in general, confident at each step. Enlarging the beam size leads to performance deterioration. Having a beam size of one (effectively a greedy decoding) also leads to sub-optimal translations.

\subsection{Czech to English}
For evaluation purposes, we take the acoustic model from the \cref{asr:transfer_phonemes}. This model has Phoneme Word Error Rate (PWER) 8.94 \% on the phonemized Corpus of Czech Parliament Plenary Hearings.

We compare our proposal with a more typical setup --- a standard ASR followed by an NMT. We employ the best performing ASR model from \cref{asr:crosslingual_intermediate}. This model has a Word Error Rate of 7.64 \% on the Corpus of Czech Parliament Plenary Hearings.

\cref{tab:eval_slt} contains SacreBLEU scores for Czech to English SLT. Translation samples are in \cref{tab:cs_en_names,tab:cs_en_sample}.

In the SLT evaluation --- translation source are transcripts obtained from the ASR --- the proposed system outperforms the baseline. We observe, the baseline struggles to predict casing and punctuation correctly. We also evaluate the translations with stripped punctuation and lower-cased. The margin narrows down, but still, the proposed system outperforms the baseline.

When tested on clean input --- a translation task --- both models perform almost equivalently. Grapheme model is slightly better when the source is enriched with punctuation and letter casing. On the other hand, phonemes do not have upper-case counterparts. Additionally, \texttt{phonemizer} do not output punctuation. Hence, we also compare a setup, where the graphemes model's input is lower-cased and without punctuation. In this case, the phoneme translation model surpasses the traditional NMT. 

A compelling observation is how the proposed model handles the proper nouns. In \cref{tab:cs_en_names}, we see that the baseline model is incapable of distinguishing the proper names. Instead, the baseline tries to find the ``closest'' common noun. On the other hand, the proposed system seems to be capable of detecting the proper noun. Although the spelling is not always correct, it is at least phonetically similar.

\paragraph{Manual evaluation}
We also assess the translation quality obtained in the SLT evaluation (the source is sound, transcribed, and then translated). 

We design the evaluation procedure as a blind experiment. We proceed as follows:

\begin{itemize}
	\item target sentences together with hypotheses of proposed and baseline system are de-punctuated and lower-cased (as the baseline struggles to capitalize and punctuate correctly),
	\item each triplet (target and the two hypotheses) are printed on three consecutive lines,
	\item order of hypotheses is randomly switched for each triplet (to prevent favoring one particular system across the test set),
	\item an evaluator can choose an arbitrary triplet for evaluation, 
	\item the evaluators are advised to ignore data points with both incorrect candidate translations,
	\item they are instructed first to read the hypotheses,
	\item finally, they should select a better suitable translation considering these criteria (in the given order):
	\begin{itemize}
		\item content,
		\item validity, or at least, phonological similarity of proper nouns,
		\item grammar.
	\end{itemize}
\end{itemize}

We employ two volunteers. We (the author) also do the evaluation. Final votes distribution is in the \cref{tab:cs_en_manual}. All judges chose the proposed as better performing. Particularly, they marked in 64 \% (volunteer 1), 69 \% (volunteer 2), and 72 \% (author) cases the translations of the proposed system as better.

\begin{table}[]
	\centering
	\begin{tabular}{l|cc|c}
		& Proposed & Baseline & Total \\ \hline
		Evaluator 1 & 14       & 8   & 22     \\ 
		Evaluator 2 & 11       & 5   & 16 \\
		Author      & 24       & 9   & 33     \\\hline
		Sum         & 49       & 23  &
	\end{tabular}
	\caption{Manual evaluation of translation quality. Each entry is the number of votes.}
	\label{tab:cs_en_manual}
\end{table}


\begin{table}[]
	\centering
	\small
	\begin{tabular}{l|llll}
		\textbf{Original} & Sally Anne Bowman & Martin Ráž  & Karim Rashid     & Karel Štědrý       \\ \hline
		\textbf{Proposed} & Sally Ambomen     & Martinarase & Cary Rashid      & Karl the Generous  \\
		\textbf{Baseline} & server ambamen    & marathon    & carym to stir up & benevolent channel
	\end{tabular}
	\caption{Samples of names from Czech to English SLT.}
	\label{tab:cs_en_names}
\end{table}

\begin{table}[]
	\centering
	\begin{tabular}{ll}
		\multicolumn{1}{l|}{\textbf{Source}}   & V zahraničí Češi spotřebují více ze svého tarifu než doma.      \\
		\multicolumn{1}{l|}{\textbf{Target}}   & Czechs consume more of their tariffs abroad than at home. \\
		\multicolumn{1}{l|}{\textbf{Proposed}} & Czechs consume more from their tariffs than at home.      \\
		\multicolumn{1}{l|}{\textbf{Baseline}} & foreign what is more used by their tariff than by home    \\
		\textbf{}                              &                                                           \\
		\multicolumn{1}{l|}{\textbf{Source}}   & Jsou to novodobí nepřátelé svobody.      \\
		\multicolumn{1}{l|}{\textbf{Target}}   & They are the modern enemies of freedom.      \\
		\multicolumn{1}{l|}{\textbf{Proposed}} & They're modern-day enemies of freedom.                 \\
		\multicolumn{1}{l|}{\textbf{Baseline}} & are the New Age of Enemy Freedom      \\
		\textbf{}                              &                                                           \\
		\multicolumn{1}{l|}{\textbf{Source}}   & Na druhém místě v žebříčku spotřebovaných dat se pak umístila Itálie.      \\
		\multicolumn{1}{l|}{\textbf{Target}}   & Italy ranked second in the list of consumed data. \\
		\multicolumn{1}{l|}{\textbf{Proposed}} & Italy ranked second in the data rankings.      \\
		\multicolumn{1}{l|}{\textbf{Baseline}} & then Italy was placed in the second place in the data used up ladder   
	\end{tabular}
	\caption{Sentence samples from Czech to English SLT.}
	\label{tab:cs_en_sample}
\end{table}


\begin{landscape}
	\begin{table}[]
		\centering
		\begin{tabular}{c|cccc|ccc|ccc}
			\multicolumn{1}{l|}{} &
			\multirow{3}{*}{\textbf{Model}} &
			\multirow{3}{*}{\textbf{Souce}} &
			\multirow{3}{*}{\textbf{Souce error}} &
			\multirow{3}{*}{\textbf{Target}} &
			\multicolumn{3}{c|}{\textbf{Cased, interpuct.}} &
			\multicolumn{3}{c}{\textbf{Uncased, no interpunct.}} \\
			\multicolumn{1}{l|}{}  &       &       &           &          & \multicolumn{3}{c|}{Beam Size} & \multicolumn{3}{c}{Beam Size} \\
			\multicolumn{1}{l|}{}  &       &       &           &          & 1        & 4        & 16       & 1        & 4        & 16       \\ \hline
			\multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{SLT}\\ (ASR source)\end{tabular}}} &
			P2G &
			ASR Phon &
			PWER 34.33 \% &
			\multirow{2}{*}{En Graph} &
			18.74 &
			\textbf{19.38} &
			18.69 &
			18.74 &
			\textbf{19.37} &
			18.57 \tabspace{16pt}\\
			\multicolumn{1}{c|}{} &  baseline & ASR Graph & WER 34.51 \% &   & 14.85    & 15.28    & 14.77    & 18.41    & 18.74    & 18.06    \\[0.2\normalbaselineskip] \hline
			\multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Translation}\\ (clean source)\end{tabular}}} &
			P2G &
			Clean Phon &
			\multirow{3}{*}{0 \%} &
			\multirow{3}{*}{En Graph} &
			29.06 &
			29.8 &
			28.73 &
			29.51 &
			29.9 &
			28.81 \tabspace{16pt}\\
			
			\multicolumn{1}{c|}{} &  baseline & ASR-like$\dagger$ Graph & & & 22.79    & 23.49    & 22.52    & 28.44    & 29.05    & 28.17  \\
			
			\multicolumn{1}{c|}{} & baseline  & Clean Graph & & & 30.73 & \textbf{31.55} & 30.87    & 30.17 & \textbf{30.89} & 30.11    
		\end{tabular}
		\caption{Evaluation of the proposed Czech to English model (phonemes to graphemes --- P2G) and the Czech to English baseline (graphemes to graphemes). We evaluate performance on SLT and Translation task. SLT task obtained source from ASR transcripts. Translation task is done on clean (original) source.\\$\dagger$ ASR-like Graph is original lowercase source with stripped interpunction.}
		\label{tab:eval_slt}
	\end{table}
\end{landscape}

\subsection{English to Czech}
We take the English acoustic model (an ASR to phonemes) from the \cref{asr:transfer_phonemes}. The model has a Phoneme Word Error Rate (PWER) 4.18 \% on Libri Speech test clean and 11.48 \% on the test other.

Baseline ASR model is a Jasper trained on Libri Speech and Common Voice. We downloaded the model from the NVIDIA NGC\footnote{\url{https://ngc.nvidia.com/catalog/models/nvidia:multidata\%20set_jasper10x5dr}}.

\cref{tab:eval_slt_en_cs} contains SacreBLEU scores for English to Czech SLT. Translation samples are in \cref{tab:en_cs_names,tab:en_cs_sample}.


Similarly, as the Czech to English SLT, the proposed system with phonemes as an intermediate step outperforms the baseline. The same goes for the translation task (clean source). Grapheme model is slightly better when the source is enriched with punctuation and letter casing. 

Manual examination of sources reveals dramatically better translation quality compared to the Czech to English. We assume it is the consequence of the quality of the acoustic and ASR models. The acoustic and ASR model performs drastically better for English than for Czech. The (Phoneme) Word Error Rate is almost halved. This is fascinating, mainly as the Czech recordings are native and the English are non-native. A probable explanation could be the amount and monotony of the Czech training data --- 400 hours of parliament speeches vs. 2000 hours. 

The handling of the proper nouns is better in both systems (compared to the Czech to English). Presumably, this might be the result of a better acoustic model.


\begin{table}[]
	\centering
	\small
	\begin{tabular}{l|llll}
		\textbf{Original} & Sally Anne Bowman & Martin Ráž  & Karim Rashid     & Karel Štědrý       \\ \hline
		\textbf{Proposed} & Sally a Bouman     & Markinrash & Carm Rushs      & Karašgibri  \\
		\textbf{Baseline} & Sully a Bowmanová    & markinrashové    & kamuflážemi & Karoskibri
	\end{tabular}
	\caption{Samples of names from English to Czech SLT.}
	\label{tab:en_cs_names}
\end{table}

\begin{table}[]
	\centering
	\begin{tabular}{ll}
		\multicolumn{1}{l|}{\textbf{Source}}   & We have to be, otherwise it doesn't work.      \\
		\multicolumn{1}{l|}{\textbf{Target}}   & Musíme být, jinak to nejde. \\
		\multicolumn{1}{l|}{\textbf{Proposed}} & Musíme být, jinak to nefunguje.      \\
		\multicolumn{1}{l|}{\textbf{Baseline}} & musíme být, jinak to nebude fungovat.    \\
		\textbf{}                              &                                                           \\
		\multicolumn{1}{l|}{\textbf{Source}}   & \makecell[l]{ Are you still optimistic about the changes\\ that have taken place within Sparta? }    \\
		\multicolumn{1}{l|}{\textbf{Target}}   & Jste stále optimistický ohledně změn, které se ve Spartě udály?      \\
		\multicolumn{1}{l|}{\textbf{Proposed}} & Jste stále optimistický ohledně změn, ke kterým došlo ve Spatě?                 \\
		\multicolumn{1}{l|}{\textbf{Baseline}} & jste stále optimističtí ohledně změn, ke kterým došlo v rámci      \\
		\textbf{}                              &                                                           \\
		\multicolumn{1}{l|}{\textbf{Source}}   & \makecell[l]{The others are Nicaragua, the Dominican Republic, El Salvador,\\ Haiti, Malta and Honduras. }     \\
		\multicolumn{1}{l|}{\textbf{Target}}   & \makecell[l]{Dalšími jsou Nikaragua, Dominikánská republika, Salvador,\\ Haiti, Malta a Honduras.} \\
		\multicolumn{1}{l|}{\textbf{Proposed}} & \makecell[l]{Ostatní Annikatagua, Nominikánská republika, lsamana,\\ Heiti Matta a Honduras.    }  \\
		\multicolumn{1}{l|}{\textbf{Baseline}} & \makecell[l]{ostatní nikaragua, nominovaná republika Elsamada\\ haiti mata a Honduras   }
	\end{tabular}
	\caption{Sentence samples from English to Czech SLT.}
	\label{tab:en_cs_sample}
\end{table}


\begin{landscape}
	\begin{table}[]
		\centering
		\begin{tabular}{c|cccc|ccc|ccc}
			\multicolumn{1}{l|}{} &
			\multirow{3}{*}{\textbf{Model}} &
			\multirow{3}{*}{\textbf{Souce}} &
			\multirow{3}{*}{\textbf{Souce error}} &
			\multirow{3}{*}{\textbf{Target}} &
			\multicolumn{3}{c|}{\textbf{Cased, interpuct.}} &
			\multicolumn{3}{c}{\textbf{Uncased, no interpunct.}} \\
			\multicolumn{1}{l|}{}  &       &       &           &          & \multicolumn{3}{c|}{Beam Size} & \multicolumn{3}{c}{Beam Size} \\
			\multicolumn{1}{l|}{}  &       &       &           &          & 1        & 4        & 16       & 1        & 4        & 16       \\ \hline
			\multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{SLT}\\ (ASR source)\end{tabular}}} &
			P2G &
			ASR Phon &
			PWER 18.61 \% &
			\multirow{2}{*}{Cs Graph} &
			16.14 &
			\textbf{16.63} &
			16.43 &
			14.03 &
			\textbf{14.35} &
			14.26 \tabspace{16pt}\\
			\multicolumn{1}{c|}{} &  baseline & ASR Graph & WER 18.80 \% &   & 13.34    & 13.85    & 13.93    & 13.34    & 13.85    & 14.00    \\[0.2\normalbaselineskip] \hline
			\multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Translation}\\ (clean source)\end{tabular}}} &
			P2G &
			Clean Phon &
			\multirow{3}{*}{0 \%} &
			\multirow{3}{*}{Cs Graph} &
			21.30 &
			21.71 &
			21.37 &
			19.17 &
			19.64 &
			19.14 \tabspace{16pt}\\
			
			\multicolumn{1}{c|}{} &  baseline & ASR-like$\dagger$ Graph & & & 16.15    & 16.86    & 16.87    & 16.87    & 17.59    & 17.55  \\
			
			\multicolumn{1}{c|}{} & baseline  & Clean Graph & & & 23.46 & \textbf{23.87} & 23.67    & 20.75 & \textbf{20.95} & 20.61    
		\end{tabular}
		\caption{Evaluation of the proposed English to Czech model (phonemes to graphemes --- P2G) and the English to Czech baseline (graphemes to graphemes). We evaluate performance on SLT and Translation task. SLT task obtained source from ASR transcripts. Translation task is done on clean (original) source.\\$\dagger$ ASR-like Graph is original lowercase source with stripped interpunction.}
		\label{tab:eval_slt_en_cs}
	\end{table}
\end{landscape}

\section{Conclusion}
\label{slt:conclusion}
We proposed an SLT system. We successfully trained the models for English and Czech (in both directions). Further, we compared these models with baselines. We achieved a higher SacreBLEU score. Also, we assessed the translation quality using human evaluation. Again, we proved the finer quality of the proposed system.

In the future, we would like to review the adaptation of the system to the ASR errors. Also, other techniques known from Neural Machine Translation can be applied (e.g., backtranslation). We are convinced, the proposed systems are capable of even better translation quality. 