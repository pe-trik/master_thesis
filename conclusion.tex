\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
\label{chap:conclusion}

In this thesis, we explored automatic speech recognition and spoken language translation. More recent studies tend to focus on end-to-end systems, but in this work, we experimented with the more traditional two-step approach of ASR and MT. As opposed to the conventional setup, we use phonemes instead of graphemes as an intermediate representation of speech.

We focus on many details of the proposed approach. First, we examine methods of speeding up training and promoting the final quality of the trained model. We utilize cross-lingual transfer from English to Czech (notably, an unrelated language). To streamline the transfer, we propose our technique --- a coarse-to-fine simplification. We proved this method provides a benefit of faster convergence and the better final result compared to both clean Czech training and na\"ive transfer. Furthermore, this technique also works entirely standalone, outperforming all other setups. We submitted these findings to the Interspeech conference (currently under review).

Further, we build a phoneme acoustic model. To speed up the training, we use transfer from traditional ASR.

An integral part of our ASR/SLT pipeline is the ``translation'' model. In ASR, this model translates phoneme sequences into grapheme transcripts in the same language. In SLT, the model does the direct translation from the source language phonemes into grapheme translation in the target language. 

As our task --- the phoneme-to-grapheme translation --- is somewhat nontraditional, we experiment with different word segmentation. More precisely, we review the size of the byte pair encoding vocabulary and different sources of training data for the BPE. We found that the best option for a high-resource setting (ours) is the larger vocabulary (32k). The source of training data did not make much difference (we tested clean, corrupt, and mixed setups).

Based on our previous findings, we trained a baseline ASR translation model. We utilized clean training data (phoneme sequences obtained using \texttt{phonemizer} tool) without the introduction of any errors. We have proved that this pipeline in the ASR task performs similarly to the end-to-end model Jasper on conventional datasets (LibriSpeech and Common Voice). We observed very promising results on our \texttt{read-newstest}, on which our ASR outperforms the end-to-end baseline model. This test set is particularly challenging, as it contains many proper nouns, and a non-native speaker reads the English part. Furthermore, we submitted our ASR system to the International Conference on Spoken Language Translation to the Non-Native Speech Translation track. On the track's development set, our systems outperformed commercial ASR systems by Google and Microsoft (see \perscite{iwslt20-polak} for more details).

We also compare the refactored two-step method with the traditional one. We compare the performance of both systems on our own \texttt{read-newstest}. For the comparison, we utilize automatic metric SacreBLEU, but we also manually asses the quality. In terms of SacreBLEU, the refactored approach outperforms the baseline. In the manual evaluation, the proposed approach outperforms the baseline in Czech to English and is slightly worse than the baseline in the opposite direction (in the number of correct translations). On the other hand, in both directions, the proposed system has more correct and ``almost'' correct translations.

Besides establishing baselines, we also experimented with enhancing the pipeline robustness. We review the transfer from SLT and fine-tune the ``translation'' system on corrupted data, to promote its ability to recover errors introduced in the acoustic model. We were able to reduce the WER on all test sets compared to the baseline.

Finally, we explored adaptation on the fly. We trained an adaptation model that is able to reduce extremely high WER (50 \% to 10 \%) on artificially corrupted phoneme transcripts. However, we were unable to obtain any enhancement on real transcripts. A probable cause of this is extreme variance in errors in the real data. We think the best possible option, how to recover errors introduced in the acoustic model, is to prepare a robust translation model. 

In summary, we are strongly convinced that the refactored two-step approach with phonemes as the intermediate representation has great potential in robust ASR and SLT. Therefore, in the future, we would like to explore this area more thoroughly. Among others, we would like to:

\begin{itemize}
	\item review and generalize the coarse-to-fine transfer to other languages and alphabets,
	\item simplify the fine-tuning of ASR, automate the creation of ``corrupted'' data that could also be used during the training of SLT,
	\item multi-task learning of ASR and SLT (with shared encoders/decoders),
	\item ``onlinezation'' of the whole pipeline with an emphasis on low latency by providing a partial translation of sentences,
	\item introduce abstract features to transport non-verbal cues from the acoustic to the translation model. This would provide a similar experience as in the end-to-end system, with the convenience of the two-step setup.
\end{itemize}