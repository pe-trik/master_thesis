\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
\label{chap:conclusion}

In this thesis, we explored automatic speech recognition and spoken language translation. More recent studies tends to focus on end-to-end systems, but in this work, we experimented with the more traditional two-step approach of ASR and MT. As opposed to the conventional setup, we use phonemes instead of graphemes as intermediate representation of speech.

We focus on many details of the proposed approach. First, we examine methods of speeding up training and promoting the final quality of the trained model. We utilize cross-lingual transfer from English to Czech (notably, an unrelated language). To streamline the transfer, we propose our own technique --- a coarse-to-fine simplification. We proved this method provides a benefit of faster convergence and better final result compared to clean Czech training or na\"ive transfer. Furthermore, this technique works perfectly as standalone, outperforming all other setups. We submitted these findings to the Interspeech conference (currently under review).

Further, we build phoneme acoustic model. To speed up the training, we use transfer from from traditional ASR.

Integral part of our ASR/SLT pipeline is the ``translation'' model. In ASR, this model translates phoneme sequences into grapheme transcripts in the same language. In SLT, the model does direct translation from the source language phonemes into grapheme translation in the target language. 

As our task --- the phoneme-to-grapheme translation --- is somewhat nontraditional, we experiment with different word segmentation. More precisely, we review size of the byte pair encoding vocabulary and different sources of training data for the BPE. We found that the best option for high-resource setting (ours) is the larger vocabulary (32k). The source of training data did not make much difference (we tested clean, corrupt and mixed setups).

Based on the previous findings, we trained a baseline ASR translation model. We utilized clean training data (phoneme sequences obtained using \texttt{phonemizer} tool) without introduction of any errors. We have proved that this pipeline in ASR task performs similarly to the end-to-end model Jasper on conventional datasets (LibriSpeech and Common Voice). We observed very promising results on our \texttt{read-newstest}, on which our ASR outperforms the baseline end-to-end model. This test set is particularly challenging, as it contains many proper nouns and the English part is read by a non-native speaker. Furthermore, we submitted our ASR system to the International Conference on Spoken Language Translation to the Non-Native Speech Translation track. On the track's development set, our systems outperformed commercial ASR systems by Google and Microsoft (see \perscite{iwslt20-polak} for more details).

We also compare the refactored two-step method with the traditional one. We compare the performance of both systems on our own \texttt{read-newstest}. For the comparison, we utilize automatic metric SacreBLEU, but we also manually asses the quality. In terms of SacreBLEU, the refactored approach outperforms the baseline. In the manual evaluation, the proposed approach outperforms the baseline in Czech to English, and worse than the baseline in the opposite direction. On the other hand, in both directions the proposed system has more correct and ``almost'' correct translations.

Besides establishing baselines, we also experimented with enhancing the pipeline. We trained the ``translation'' system on corrupted data, to promote its ability to recover errors introduced in the acoustic model. We were able to reduce the WER on all test sets compared to the baseline.

Finally, we explored adaptation on the fly. We trained an adaptation model that is able to reduce extremely high WER (50 \% to 10 \%) on artificially corrupted phoneme transcripts. However, we were unable to obtain any enhancement on real transcripts. The probable cause of this is extreme variance in errors in the real data. We think, the best possible option, how to recover errors introduced in the acoustic model is to prepare a robust translation model. 

In summary, we are strongly convinced that the refactored two-step approach with phonemes as intermediate representation has great potential in robust ASR and SLT. Therefore, in the future, we would like to explore this area more thoroughly. Among others, we would like to:

\begin{itemize}
	\item review and generalize the coarse-to-fine transfer to other languages and alphabets,
	\item simplify the fine-tuning of ASR, automate the creation of ``corrupted'' data that could be also used during the training of SLT,
	\item multi-task learning of ASR and SLT (with shared encoders/decoders),
	\item introduce abstract features to transport non-verbal cues from the acoustic to the translation model. This would provide similar experience as in the end-to-end system, with the convenience of the two-step setup.
\end{itemize}