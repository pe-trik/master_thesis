\chapter{Automatic Speech Recognition}
\label{chapter:asr}

This chapter is devoted to automatic speech recognition (ASR). Throughout this chapter, we describe in detail our modifications to the process of training an ASR pipeline. One of the central challenges of this work is the scarcity of Czech training data. We propose a technique to overcome this problem. 

This chapter is organized as follows: \cref{asr:crosslingual_intermediate} presents and compares two techniques for overcoming the shortage of Czech ASR data. \cref{asr:transfer_phonemes} describes transfer learning from graphemes to phonemes and the training of acoustic models for the final ASR/SLT pipeline. 


\section{Coarse-to-Fine Intermediate Step and Cross-Lin\-gual ASR Transfer}
\label{asr:crosslingual_intermediate}

In this section, we review techniques for overcoming training data shortage. Note, this section was submitted in the format of a paper to the Interspeech conference and it is now under review.

\subsection{Introduction}

Contemporary end-to-end, deep-learning automatic speech recognition systems achie\-ved state-of-the-art results on many public speech corpora, see e.g. \perscite{han2019state}
on test-clean set from LibriSpeech.

To outperform traditional hybrid models, deep-learning ASR systems, however, must be trained on vast amounts of training data, on the order of a thousand of hours. Currently, there is only a limited number of public datasets that meet these quantity criteria. The variety of covered languages is also minimal. In fact, most of these large datasets contain only English. Although new speech datasets are continually emerging, producing them is a tedious and expensive task.

Another downside of new end-to-end speech recognition systems is their requirement of an extensive computation on many GPUs, taking several days to converge, see e.g., \perscite{karita2019comparative}. 

These obstacles are often mitigated with the technique of transfer learning \parcite{tan2018survey} when a trained model or a model part is reused in a more or less related task.
%make the research and use of a contemporary end-to-end, neural networks inaccessible for many practitioners.
Furthermore, it became customary to publish checkpoints alongside with the neural network implementations and 
there emerge repositories with pre-trained neural networks such as \textit{TensorFlow Hub}\footurl{https://tfhub.dev/} or \textit{PyTorch
	Hub}.\footurl{https://pytorch.org/hub/} This gives us an opportunity to use pre-trained models, but similarly, most of the published checkpoints are trained for English speech.

In our work, we propose a cross-lingual coarse-to-fine intermediate step and experiment with transfer learning \parcite{tan2018survey}, i.e., the reuse of pre-trained models for other tasks. Specifically,
we reuse the available English ASR checkpoint of QuartzNet \parcite{kriman2019quartznet} and train it to recognize Czech speech instead.

This section is organized as follows. In \cref{asr:related_work}, we give an overview of related work. In \cref{sec:data_models} we describe the used models and data. Our proposed method is described in \cref{sec:experiments}, and the results are presented and discussed in \cref{sec:results}.
Finally, in \cref{sec:conclusion} we summarize the work%, and we give a brief outlook of our future plans.


\subsection{Related Work}
\label{asr:related_work}

Transfer learning \parcite{tan2018survey} is an established method in machine learning because many tasks do not have enough training data available, or they are too computationally demanding. In transfer learning, the model of interest is trained with the help of a more or less related ``parent'' task, reusing its data, fully or partially trained model, or its parts.

Transfer learning is step by step becoming popular in various areas of NLP. 
For example, transferring some of the parameters from parent models of high-resource languages to low-resource ones seems very helpful in machine translation \parcite{zoph-etal-2016-transfer,kocmi-bojar-2018-trivial}.

Transfer learning in end-to-end ASR is studied by  \perscite{kunze-etal-2017-transfer}. They show that (partial) cross-lingual model adaptation is sufficient for obtaining good results. Their method exploits the layering of the architecture. In essence, they take an English model and freeze weights in the upper part of the network (closer to the input). Then they adapt the lower part for German speech recognition yielding very good results while reducing training time and the amount of needed transcribed German speech data.

Other works concerning end-to-end ASR are \perscite{TONG201839} and \perscite{kim}. The former proposes unified IPA-based phoneme vocabulary while the latter suggests a universal character set. The first demonstrates that the model with such alphabet is robust to multilingual setup and transfer to other languages is possible. The latter proposes language-specific gating enabling language switching that can increase the network's power.

Multilingual transfer learning in ASR is studied by \perscite{cho2018multilingual}. First, they jointly train one model (encoder and decoder) on ten languages (approximately 600 hours in total). Second, they adapt the model %, encoder, and decoder,
for a particular target language (4 languages, not included in the previous 10, with 40 to 60 hours of training data). They show that adapting both encoder and decoder, boosts performance in terms of character error rate.

Coarse-to-fine processing \parcite{raphael:coarse-to-fine:2001} has a long history in NLP. It is best known in the parsing domain, originally applied for the surface syntax \parcite{charniak:etal:2006} and recently for neural-based semantic parsing \parcite{dong-lapata-2018-coarse}. The idea is to train a system on a simpler version of the task first and then gradually refine the task up to the desired complexity. With neural networks, coarse-to-fine training can lead to better internal representation, as e.g., \perscite{coarse-to-fine-nmt:word-repr:2018} observe for neural machine translation.

The term coarse-to-fine is also used in the context of hyperparameter optimization, see e.g., \perscite{coarse-to-fine-hyperparam:2017} or the corresponding DataCamp class,\footnote{\url{https://campus.datacamp.com/courses/hyperparameter-tuning-in-python/informed-search?ex=1}} to cut down the space of possible hyperparameter settings quickly.

Our work is novel and differs from the abovementioned in two ways: First, we reuse existing models and checkpoints to improve the speed and accuracy of an unrelated language. Second, in the coarse-to-fine step method, we simplify (instead of unifying) Czech character set in order to improve cross-lingual transfer, but also to enhance monolingual training significantly.


\subsection{Data and Models Used}
\label{sec:data_models}

%We build upon the following components:

\paragraph{Pre-Trained English ASR.}

As the parent English model we use the checkpoint available at the \textit{NVIDIA GPU Cloud}.\footurl{https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5} It is trained for 100 epochs with batch size 512 on 8 NVIDIA V100 GPUs and achieves 3.98\,\% WER on LibriSpeech \parcite{panayotov2015librispeech} test-clean.

During the experiments, the model configuration provided by the NeMo authors is used with minor changes (we used 1000 warm-up steps and learning rate $10^{-3}$ for decoder adaptation). We note that we use $O1$ optimization setting. Which primarily means mixed-precision training (weights are stored in single precision, gradient updates are computed in double precision). We perform training on 10 NVIDIA GeForce GTX 1080 Ti GPUs with 11 GB VRAM.


\paragraph{Czech Speech Data.}
In our experiments, we use Large Corpus of Czech Parliament Plenary Hearings \parcite{dataset}. At the time of writing, it is the most extensive available speech corpus for the Czech language, consisting of approximately 400 hours.

The corpus includes two held out sets: the development set extracted from the training data and reflecting the distribution of speakers and topics, and the test set which comes from a different period of hearings. We choose the latter for our experiments because we prefer the more realistic setting with a lower chance of speaker and topic overlap.

A baseline, end-to-end Jasper model, trained on this corpus for 70 epochs, has an accuracy of 14.24\,\% WER on the test set.


\subsection{Examined Configurations}
\label{sec:experiments}


\begin{figure*}[t]
	\centering
	\resizebox{\columnwidth}{!}{%
    \begin{tikzpicture}[thick, node distance=4cm, 
          >=stealth,
          bend angle=45,
          auto]
          \def\y{0.28284271247461900976033774484193961571393437507539};
          \def\revgap{5.5};
        \draw
            node at (0,0)[block, name=en]{\shortstack{\Large EN\\ \small{checkpoint}}}
            node [block, below right =\y cm of en] (scz) {\shortstack{simplified\\ \Large CS}}
            node [block, right of=scz] (cz1) {\shortstack{\Large CS\\ \small{decoder adaptation}}}
            node [block, right of=cz1] (cz) {\Large CS}
            
            node [block, above =0.2cm of cz1] (cz_e1) {\shortstack{\Large CS\\ \small{decoder adaptation}}}
            node [block, above =0.2cm of cz] (cz_e) {\Large CS}
            
            node [block, below =0.2cm of cz1] (ccz1) {\shortstack{\Large CS\\ \small{decoder adaptation}}}
            node [block, left of=ccz1] (cscz) {\shortstack{simplified\\ \Large CS}}
            node [block, below =0.2cm of cz] (ccz) {\shortstack{\Large CS\\ \small{ }}};
            
            \draw[->](en) |- node[below] {\small\shortstack{encoder\\decoder}} (scz);
            \draw[->](scz) -> node {\small encoder} (cz1);
            \draw[->](cz1) -> node {\small\shortstack{encoder\\decoder}} (cz);
            
            \draw[->](en)  -> node {\small encoder} (cz_e1);
            \draw[->](cz_e1) -> node {\small\shortstack{encoder\\decoder}} (cz_e);
            
            \draw[->](cscz) -> node {\small encoder} (ccz1);
            \draw[->](ccz1) -> node {\small\shortstack{encoder\\decoder}} (ccz);

            \node [left = \revgap cm of cz_e1] {Basic Transfer:};
            \node [left = \revgap cm of cz1] {Transfer+Vocab. Simpl:};
            \node [left = \revgap cm of ccz1] {Vocab. Simpl. Only:};
    \end{tikzpicture}
	}
	\caption[Examined setups of transfer learning]{Examined setups of transfer learning. The labels on the arrows indicate which model parts are transferred, i.e., used to initialize the subsequent model. No parameter freezing is involved except for the encoder weights in the ``CS decoder adaptation'' phase.}
	\label{fig:transfers}
\end{figure*}

\cref{fig:transfers} presents the examined setups. In all cases, we aim at the best possible Czech ASR, disregarding the performance of the model in the original English task. The baseline (not in the diagram) is to train the network from scratch on the whole Czech dataset, converting the speech signal directly to Czech graphemes, i.e., words in fully correct orthography, except punctuation and casing which are missing in both the training and evaluation data.

\subsubsection{Basic Transfer Learning}
\label{basic_transfer}

The first method is very similar to \perscite{kunze-etal-2017-transfer}. We use the English checkpoint with the (English) WER of 3.98\,\% on LibriSpeech test-clean, and continue the training on Czech data.

Czech language uses an extended Latin alphabet, with diacritic marks (acute, caron, and ring) added to some letters. This extended alphabet has 42 letters, including the digraph ``ch''. Ignoring this digraph (it is always written using the letters ``c'' and ``h''), we arrive at 41 letters. Only 26 of them are known to the initial English decoder.

To handle this difference, we use a rapid decoder adaptation. For the first 1500 steps, we keep the encoder frozen and train the decoder only (randomly initialized; Glorot uniform).

Subsequently, we unfreeze the encoder and train the whole network on the Czech dataset.

\subsubsection{Transfer Learning with Vocabulary Simplification}

In this experiment, we try to make the adaptation easier by first keeping the original English alphabet and extending it to the full Czech alphabet only once it is trained.

To coerce Czech into the English alphabet, it is sufficient to strip diacritics (e.g. convert ``\v{c}\'arka'' to ``carka''). This simplification is quite common in Internet communication but it always conflates two sounds (\textipa{[ts]} written as ``c'' and \textipa{[tS]} written as ``\v{c}'')  or their duration (\textipa{[a:]} for ``\'a'' and \textipa{[a]} for ``a'').

In this experiment, we first initialize both encoder and decoder weights from the English checkpoint (English and simplified Czech vocabularies are identical so the decoder dimensions match), and we train the network on the simplified Czech dataset for 40 thousand steps.

The rest (adaptation and training on the full Czech alphabet)
is the same as in \cref{basic_transfer}.
%
Overall, this can be seen as a simple version of coarse-to-fine training where a single intermediate model is constructed with a reduced output alphabet.

\subsubsection{Vocabulary Simplification Only}
\label{sub_sec:simplification}

% In this experiment, we first simplify target vocabulary: we use standard Latin alphabet with 26 letters plus space and apostrophe (to preserve compatibility with English). Czech transcripts are then encoded using this simplified alphabet (e.g. ``\v{c}\'arka'' as ``carka'').

%With transcripts encoded in this manner, we train a randomly (Glorot uniform) initialized QuartzNet network for 40 thousand steps. 

%From our previous experience with vocabulary adaptation, we make a brief adaptation of the model for a different alphabet. We initialize encoder with weights obtained in the previous step and modify the target vocabulary to all Czech letters (41 plus space and apostrophe). The decoder is initialized with random weights. We freeze the encoder and train this network shortly for 1500 steps.

%After this brief adaptation step, we unfreeze the encoder and train the whole network for 40 thousand steps.

In the last experiment, we disregard the English pre-trained model and use only our vocabulary simplification trick. We first train the randomly initialized model on Czech without diacritics (26 letters) for 38 thousand steps. We then adapt the pre-trained, simplified Czech model to the full Czech alphabet with diacritics (41 letters), again via the short decoder adaptation. Note that the original decoder for simplified Czech is discarded and trained from random initialization in this adaptation phase.



\begin{table}[t]
	\small\centering
	\begin{tabular}{lc|cc}
		%\hline 
		\bf Experiment & \bf Simplified & \bf Adaptation phase & \bf Full training \\
		\hline
		Baseline CS & - &  - &  20.19 \\
		%\hline
		EN $\rightarrow$ CS & -  & 97.35 &  19.06  \\
		%\hline
		EN $\rightarrow$ sim. CS $\rightarrow$ CS & 17.11  & 22.01 &  16.88 \\
		%\hline
		sim. CS $\rightarrow$ CS & 20.56  &  24.59 &  16.57  \\
		%\hline
	\end{tabular}
	\caption[Results in \% of WER on the Czech (CS) test set]{Results in \% of word error rate on the Czech (CS) test set. ``Simplified''
    column reflects WER after the training on simplified dataset (both training and test data
    without accents). ``Adapt.'' column contains WER 
    immediately after the decoder adaptation phase to the full Czech alphabet including accents.
    Finally, ``Full'' column contains performance on the test set with accents 
    after the full training.}
	\label{tab:results}
\end{table}

\begin{landscape}
	\begin{figure}[t]
		\includegraphics[width=\linewidth,height=13cm]{img/figure}
		\caption[Evaluation on test set during CS ASR training]{Evaluation on test set during training. % (every 500 steps).
			Note, that WER curves for experiments with simplified vocabulary (thin lines) are not directly comparable with other curves until step 40,000 as the test set is on a different (simplified) vocabulary. 10,000 steps takes approximately 5 hours of time.}
		\label{fig:training}
	\end{figure}
\end{landscape}

\subsection{Results and Discussion}
\label{sec:results}


\cref{tab:results} presents our final WER scores and \cref{fig:training} shows their development through the training. For simplicity, we use greedy decoding and no language model. We do not use a separate development set. We simply take the model from the last reached training iteration.\footnote{Little signs of overfitting are apparent for the ``Simplified CS $\rightarrow$ CS'' setup so that an earlier iteration might have worked better, but we do not have another test set with unseen speakers to validate it.}

\subsubsection{Transfer across Unrelated Languages}

We observe that initialization %of network weights
with an unrelated language helps to speed-up training.
%\XXX{Schovavam pracovni formulace nechavam jen novou:}
%This is best apparent in
%``English $\rightarrow$ Czech simplified'' \XXX{(see \cref{fig:training}, light
%green dashed line)} where the unchanged vocabulary allows us to reuse all the
%weights.
%\XXX{Neni jasne, k cemu odkazujete. Je potreba rict, ze se divate na
%Fig 2 (doufam) a ze tam je to kde? V jakem rozsahu osy X? PP: ano, Fig 2 zelena
%prerusovana. OB: nojo, ale stejne neni jasne s cim to srovnavate. Napsal bych 
%radeji neco jako:}
This is best apparent by comparing the first 40k steps of the learning curves for ``English
$\rightarrow$ Czech simplified'' and ``Czech (simplified) $\rightarrow$ Czech''
in \cref{fig:training}, thin line. Here the target alphabet was simplified in
both cases but the weights
initialized from English allow much faster decrease of WER. The benefit is clear
also for the full alphabet (baseline vs. ``English $\rightarrow$ Czech'') where
the baseline has a consistently higher WER.

% \XXX{Kdyz to nasledujici neni videt, tak se to musi napsat velmi nezne, neco
% jako: Additionally, we observed... a hodne presne to popsat, o co slo. Co
% treba:}
The training off the English parent is actually so fast that WER for the
simplified alphabet (thin dashed green line) drops under 30\,\% within the first 2000
steps (1 hour of training).
%WER drops under 30\,\% \XXX{(not visible in the figure)} after only
%2000 steps (1 hour of training).
This can be particularly useful if the final
task does not require the lowest possible WER, such as sound segmentation.

%\XXX{Puvodni formulace schovavam a prepisuju:}
% Basic transfer, when the target alphabet is altered prior the training
% (``English $\rightarrow$ Czech''), boosts the convergence at the beginning of
% the training. 
% %Our setting with QuartzNet and as well as these results are similar to
% %\cite{kunze-etal-2017-transfer} with a high $k = 18$. % abstracted to
% %QuartzNet's convolutional layers $C_1$ to $C_4$ and blocks $B_i$ with high $k =
% %18$.} 
% However, this advantage diminishes with longer training, gaining only
% %becomes weaker as the training progresses. Towards the end, the advantage is
% %roughly
% 1 to 2\,\% points of WER over the baseline in the end.
% 
% It seems, though, that in the experiments with vocabulary simplification, 
% %after the change to the full Czech vocabulary, 
% the results are better for a randomly
% initialized network (WER 16.57\,\%), rather than the one with transfer from
% English (WER 16.88\,\%) (see \cref{tab:results}).
While Basic transfer (``English $\rightarrow$ Czech'') boosts the convergence
throughout the training, its final performance is only 1 to 2\,\% points of WER
better than the baseline, see the plot or compare 20.19 with 19.06 in
\cref{tab:results}. The intermediate vocabulary simplification is more important
and allow a further decrease of 2.2\,\% WER absolute (19.06 vs. 16.88) when transferring
from English.

\subsubsection{Transfer across Target Vocabularies}

In the course of two experiment runs, we altered the target vocabulary: the
training starts with simplified Czech, and after about 40,000 steps, we switch
to the full target vocabulary. This sudden change can be seen as spikes in
\cref{fig:training}. Note that WER curves prior to the peak use the simplified
Czech reference (the same test set, but with stripped diacritics), so they are
not directly comparable to the rest.

The intermediate simplified vocabulary always brings a considerable improvement. In essence, the final WER is lower by 2.18 (16.88 vs. 19.06 in \cref{tab:results}) for the models transferred from English and by 3.62 (16.57 vs. 20.19) for Czech-only runs.
One possible reason for this improvement is the ``easier'' intermediate task of
simpler Czech. Note that the exact difficulty is hard to compare as the target
alphabet is smaller, but more spelling ambiguities may arise. This intermediate
task thus seems to help the network to find a better-generalizing region in the parameter space. Another possible reason that this sudden change and reset of the last few layers allows the model to reassess and escape a local optimum in which the ``English $\rightarrow$ Czech'' setup could be trapped.


\subsection{Conclusion and Future Work}
\label{sec:conclusion}

We presented our experiments with transfer learning for automated speech recognition between unrelated languages.
In all our experiments, we outperformed the baseline in terms of speed of convergence and accuracy.

We gain a substantial speed-up when training Czech ASR while reusing weights
from a pre-trained English ASR model. The final word error rate improves over
the baseline only marginally in this basic transfer learning setup.

We are able to achieve a substantial improvement in WER by introducing an intermediate step in the style of coarse-to-fine training, first training the models to produce Czech without accents, and then refining the model to the full Czech.
This coarse-to-fine training is most successful within a single language: Our final model for Czech is better by over 3.5 WER absolute over the baseline, reaching WER of 16.57\%. %Further gains are expected from beam search with language model or better iteration choice to avoid overfitting.

%As we documented in the \cref{sec:results}, transfer learning leads to a substantial reduction in training time. We achieved speed-up even in unrelated languages. We also demonstrated that the coarse-to-fine approach leads not only to training time reduction but also yields better accuracy.

We see further potential in the coarse-to-fine training. We would like to explore this area more thoroughly, e.g., by introducing multiple simplification stages or testing the technique on more languages.


\subsection{Note on the Results}
After reviewing the Large Corpus of Czech Parliament Plenary Hearings
\cite{dataset}, we found out that the transcripts contained systematic errors
(the ``SIL'' tag for silence was accidentally left in at the beginnings and ends of some transcripts). This resulted in a systematic mismatch of the system output and the reference transcript during testing. 

Unfortunately, we made this finding too late to be able to redo all the
experiments.
%after discarding some of the models, so we were unable to re-run the test. We kept only the best model.
\cref{tab:results_rerun} compares the WER scores of the best model on the
original and fixed reference transcripts (without any model retraining). We also add scores with beam search
and a 5-gram KenLM \parcite{Heafield-kenlm} language model, confirming the improvement we expected.
The final score is thus 4.91 on the development and 6.42\,\% WER on the test
set, ten points better than the scores reported in \cref{tab:results}.

\begin{table}[t]
	\centering
	\begin{tabular}{lcc}
		%\hline 
		\bf Set & \bf Greedy & \bf Beam search with LM \\
		\hline
		Development & 5.19 & 4.91 \\
		%\hline
		Test & 7.64 & 6.42 \\
		%\hline
	\end{tabular}
	\caption[The best Czech model performance]{Our best Czech model. Results in \% WER measured after fixing errors in development and test set transcriptions.}
	\label{tab:results_rerun}
\end{table}

\pagebreak

%***********************************************************
%********************* ASR to phonemes *********************
%***********************************************************

\section{Speech Recognition to Phonemes}
\label{asr:transfer_phonemes}
In this section, we build ASR systems for English and Czech with phonemes as target vocabulary. During the training, we build upon our findings from the previous section.

This section is organized as follows: First, in \cref{asr:phon:related} we review related work and discuss the motivation for the transition from graphemes to phonemes as target vocabulary. Next, in \cref{asr:phon:experiment} we present the experiment set-up. \cref{asr:phon:training} gives details about training. Finally, we evaluate results in \cref{asr:phon:eval}.

\subsection{Related Work}
\label{asr:phon:related}

Phonemes are well-established modeling units in speech recognition. From the beginnings of the ASR technology in the 1950s, researchers employed phonemes \parcite{juang2005automatic}. 

For the GMM-HMM models, a state typically represents part of a phone \parcite{yu2016automatic}. A \emph{phone} is a linguistic unit representing a sound regardless of the meaning in a particular language. On the other hand, we have \emph{phonemes} that are also linguistic units representing a sound. However, opposed to the phones, if a phoneme is switched for another one, it could change the meaning of a word in a particular language. A common trick for improving the quality is to combine phones into \emph{diphones} (two consecutive phones) or \emph{triphones} (three successive phones) \parcite{kamath2019deep}.  

\perscite{riley1992recognizing} perform a comparison of different linguistic units for sound representation. Namely, they compare phonemic (using phones), phonetic (using phonemes), and multiple phonetic realizations with associated likelihoods. All experiments use the GMM-HMM ASR model. Using the phonemic representation, the authors were able to achieve 93.4 \% word accuracy on DARPA Resource Management Task. Using the most probable phonetic spelling, the authors improved the result to 94.1 \%. Authors also propose using more phonetic realizations for each word, allowing different pronunciations. These are obtained automatically from the TIMIT phonetic database. In their best setup, which achieves an accuracy of 96.3 \%, they report the average of 17 representations per word.

An important piece of work popularizing neural networks in ASR is that of \perscite{waibel1989phoneme}. The work proposes to use a time-delayed neural network to model acoustic-phonetic features and to model the temporal relationship between them. The authors demonstrate that the proposed NN can learn shift-invariant internal abstraction of speech and use this to make a robust final decision. The authors choose phonemes as the modeling unit.

More recent studies that still use hybrid models (mainly, HMM and DNN) employ multi-task learning to improve phone recognition task significantly. 

For example, \perscite{seltzer2013multi} improve the primary task of predicting acoustic states (61 phonemes, three states each) using a different secondary assignment. They propose three additional problems: (1) Phone Label Task, where the system predicts phone labels for each acoustic state. (2) State Context Task, in which the model predicts the previous and following acoustic state. (3) Phone Context Task, where the model predicts the previous and next phone.

Another example of multi-task learning in ASR is \perscite{chen2014joint}. The authors use a DNN for the triphone recognition task. To improve the performance in this task, they suggest using the additional task of trigrapheme recognition, which they claim to be highly related. In their setup, both problems share the internal representation (the hidden layers of DNN). The authors successfully demonstrate that their contribution outperforms other single task methods and even the ROVER system \parcite{fiscus1997post} that combines results from the triphone and trigrapheme tasks.

Finally, though not directly from the ASR field, an excellent utilization for phoneme ASR in speech translation is suggested by \perscite{salesky-etal-2019-exploring}. For their end-to-end speech translation pipeline, they first obtain phoneme alignment using the DNN-HMM system. They average feature vectors with the same phoneme for consecutive frames. Phonemes then serve as input features for end-to-end speech translation. \XXX{Salesky pouziva Kaldi a phonemy vnima ako "hustejsiu" reprezentaciu zvuku, takze cele jej SLT je tym padom E2E.}

\subsection{Experiment Set-Up}
\label{asr:phon:experiment}
Based on the abovementioned review of related work, we design our experiment. The main goal of our work is to build spoken language \emph{translation} system. This has a significant impact on our decision-making about the rough outline.

We target our ASR to phonemes rather than graphemes. We presume they are more suitable linguistic modeling units for the recognition task, while they keep the problem similar to the grapheme recognition task (mainly supported by  \perscite{riley1992recognizing,juang2005automatic,chen2014joint}). This is perhaps more true for Czech where the phonetical and graphical transcriptions are relatively alike. We specifically decided for IPA phonemes and not for other phonetic units like phones. We believe that the IPA are adequate and we can quickly get phonetical transcription for most languages using the \texttt{phonemizer}\footnote{\url{https://github.com/bootphon/phonemizer}} tool.

We find the end-to-end system architecture for ASR promising for the future. Therefore, unlike the DNN-HMM architecture used by the \perscite{salesky-etal-2019-exploring}, we bet on E2E architecture. Notably, for English, we employ the bigger Jasper architecture \parcite{Li2019}, and for Czech, we utilize the smaller QuartzNet architecture \parcite{kriman2019quartznet}. Our decision to use different architectures for English and Czech is motivated by the volume of the training data and the number of trainable parameters of the respective models. For English, we have almost 2000 hours of transcribed speech and 333 million of parameters in the Jasper model while for Czech only about 400 hours of transcribed speech and 18.9 million of parameters in the QuartzNet model.

We take advantage of the fact that the grapheme and phoneme recognition tasks are similar. We employ transfer learning from pre-trained grapheme models. Inspired by our previously proposed ``Adaptation method'' (see \cref{sec:experiments}), we first strip the ``grapheme'' decoder. We replace the stripped grapheme decoder with a new decoder with correct number of output classes to account number of phonemes. This new decoder is then randomly initialized (using Glorot uniform). During the adaptation phase we adapt the model (only a randomly initialized decoder). After the adaptation, we train the whole model (encoder and decoder together).

\subsection{Training}
\label{asr:phon:training}
In this section, we review the training of the experiment, as described in \cref{asr:phon:experiment}. Note that we have different training procedures for English and Czech. The training scheme is common to both languages: first, the Adaptation phase, and second, the Full training.

We use mixed-precision training, which means that the weights are stored in \texttt{float16} (making the model smaller), and weight updates are computed in \texttt{float32} precision (reducing the quality loss).

When it comes to phoneme segmentation, one technical question arises. Both trained architectures output character labels, but some phonemes are multi-character (for example ``\textipa{a:}'', ``\textipa{dZ}'', ``\textipa{aI}'', ``\textipa{aI@}'' or ``\textipa{aI\textrhookschwa}'',). We already dealt with this problem in the previous section due to the Czech digraph ``ch''. We decided to disregard this character in the ASR to Czech graphemes as it can be easily rewritten using ``c'' and ``h''. Here the problem is more frequent. Many phonemes in both languages are multi-character (Czech has 13 and English 27). Examples of words with different multi-character phonemes are in \cref{tab:phon_examples} on page~\pageref{tab:phon_examples}. From the examples, we see that some of the multi-character phonemes consist even of other single-character ones. Hence, disregarding this issue might influence the model performance. Bearing this in mind, we conduct two additional experiments. The first experiment obeys the phoneme segmentation, and the second breaks all phonemes to single characters. As the English model has much greater hardware requirements, we train only Czech variants and assume the results will carry over to English.

We first attempt to follow the phoneme segmentation given by the \texttt{phonemizer}. \texttt{phonemizer} can output phonetic transcriptions including separators between pho\-ne\-mes and words. We use ``$\mid$'' as the phoneme separator (see \cref{tab:phon_examples}). \XXX{Note, this separator is used by the data preparation layer in the acoustic model. The layer splits the input phoneme string on the separator symbols, and each substring is translated to one phoneme label. Hence, the neural network is unaware of the separator symbol. Specifically, the acoustic model output does not contain any separators.}

%In the second attempt, we decompose all multi-character phonemes. Additionally, we break single-character phonemes that consist of combining letters and we replace these special letters with a placeholder. For English we substitute ``\textipa{\textsuperscript{j}}'' with ``J'', combining tilde (like ``\textipa{\~O}'') with ``I'', and vertical line below (e.g., ``\textipa{\textsyllabic{n}}'') with ``L''. For Czech, we introduce placeholders for vertical line below (e.g., ``\textipa{\textsyllabic{l}}'') with ``L'', for up tack below (e.g., ``\textipa{\textraising{r}}'') with ``T'' and ring above (e.g., ``\textipa{\r{r}}'') with ``R''. So, for example Czech word ``p\v{r}\'ipadn\v{e}'' has phonetical transcription ``\textipa{p\r{\|'r}i:pad\textltailn e}'', which is rewritten to ``\textipa{pr}TR\textipa{i:pad\textltailn e}''.

In the second attempt, we decompose all multi-character phonemes (e.g., ``\textipa{dZ}'' is decomposed to ``\textipa{d}'' and ``\textipa{Z}''). Note, some of the single-character phonemes are encoded in \texttt{UTF-8} as multi-character using combining letters. To circumvent this, we additionally break single-character phonemes that consist of combining letters and we replace these special letters with a simple Latin letter substitute. The substitute letters I, L, T, R were chosen so that they do not occur in the respective language. So, for example the Czech word ``p\v{r}\'ipadn\v{e}'' with the phonetical transcription ``\textipa{p\r{\|'r}i:pad\textltailn e}'' is rewritten to ``\textipa{pr}TR\textipa{i:pad\textltailn e}''.

\paragraph{Czech ASR}
For training and testing, we use the ``phonemized'' corpus of the Czech Parliament Plenary Hearings. As the dataset is five times smaller than English corpora, we utilize the smaller QuartzNet.

We train off the best model from the previous experiment described in \cref{asr:crosslingual_intermediate}. This model yields WER of 4.91 \% on the development and 6.42 \% the test set on the Parliament corpus using beam search with decoding.

The Adaptation Step takes 2000 steps. As the model's memory footprint is smaller during this phase, we increase the batch size to 256. Two thousand steps are warm-up, the maximal learning rate is $4 \times 10^{-3}$.

The full training takes 30000 steps. The model memory requirements increase, therefore we reduce the batch size to 32. We also reduce the learning rate to $10^{-3}$.

Both experiments with phonemes alphabets (the broken and non-broken one) use the same configuration.

\paragraph{English ASR}
For the training and evaluation of English ASR, we translate to pho\-ne\-mes the corpora LibriSpeech and Mozilla Common Voice (again using the \texttt{pho\-ne\-mi\-zer} tool). We use both (shuffled) datasets for the Adaptation and Full training phases.

The training is executed on 10 NVIDIA GTX 1080 Ti GPUs with 11 GB VRAM.

We train off the Jasper ASR model as available online.\footnote{\url{https://ngc.nvidia.com/catalog/models/nvidia:multidata set\_jasper10x5dr}} This model was trained on the LibriSpeech, Mozilla Common Voice, WSJ, Fisher, and Switchboard corpora. The model yields 3.69 \% on the test-clean, and 10.49 \% on the test-other using greedy decoding on the LibriSpeech.

The Adaptation phase takes 2000 steps. As the model's memory footprint is smaller during this phase, we increase the batch size to 64 (global batch size is 640). One thousand steps are warm-up; the maximal learning rate is $4 \times 10^{-3}$.

The Full training takes ten epochs. The model memory requirements increase, therefore we reduce the batch size to 16 (global batch size is 160). We also reduce the learning rate to $10^{-3}$.

\subsection{Evaluation}
\label{asr:phon:eval}
In this section, we review the course of training and evaluate the models' performance.

Note, the results are not directly comparable with graphemes. The mapping to phonemes is not a 1-1. For example, the two words ``I am'' map in American English to only one ``phoneme word'' \textipa{[aI\ae m]}. For this reason we carefully distinguish between the standard word error rate and phoneme word error rate (PWER) which measures the same Levenshtein distance on the sequence of phoneme tokens.

\paragraph{Czech}
The course of the Czech training is displayed in \cref{fig:cs_phon} and the final evaluation is in \cref{tab:cs_phon_results}. 

As expected, a rapid PWER fall at the beginning of the training (see \cref{fig:en_phon}) proves that the grapheme and phoneme recognition tasks are indeed similar because the pre-trained weights adapt so quickly and well to the new target units.

We made a surprising observation: beam search decoding is always worse than greedy decoding. We tested the beam sizes of power two from 1 to 64. We use two different beam search implementations: TensorFlow\footnote{\url{https://www.tensorflow.org/}} \XXX{for beam search without LM rescoring} and Baidu's CTC decoder\footnote{\url{https://github.com/PaddlePaddle/DeepSpeech}} with KenLM \citep{Heafield-kenlm} language model. 

Using \XXX{beam search without LM rescoring (TensorFlow)}, we got the best result 9.14 \% PWER, about 0.2 \% percent point higher than with the greedy search for multi-character ASR. For single-character ASR, the difference is even more significant: 9.09 \% for greedy and 9.53 \% PWER for beam search. 

We also tried adding a language model to the second CTC decoder.\footnote{When setting LM weight to higher than 0, we found that this implementation does not support labels containing more that one character, which causes a problem to the several multi-character phonemes in the phonetic alphabet.} Using KenLM, we trained a 3-gram language model on phonemized Czech part of the CzEng. For both single- and multi-character phonemes, the language model helps. The result for the single-character setup is a bit better. 
\paragraph{\XXX{Phoneme segmentation discrepancy in trainig data}}
We suspect the difference stems from our trick (substituting multi-character phonemes with a new single character --- see \cref{asr:phon:training}) to overcome Baidu's CTC decoder limitation. Using this trick, we substitute multi-character phonemes with a unique symbol in transcripts (used for the acoustic model training) and language model training data.

The language model training data (unlike the transcripts) does not contain phoneme boundaries (i.e., there is no phoneme separator token ``$\mid$'' present). Some multi-character phonemes consists of two other valid single-character (or even multi-character) ones (see \cref{tab:phon_examples}). Thus, during the substitution of multi-character phonemes, some neighboring phonemes might be considered as one phoneme and incorrectly substituted. Note, the ASR model is trained on data with known phoneme segmentation (given by the \texttt{phonemizer}). Hence, during the decoding, the acoustic model can sometimes attribute a higher probability to two phonemes instead of one, while the language model gives to these phonemes lower score (because of the training data mismatch of the language and ASR model). An illustration of this problem is in \cref{tab:phon_examples}. For example, the phonetic transcriptions of ``serious'' and ``various'' are \textipa{sI\*ri@s} and \textipa{vE\*ri@s}. They both share common suffix --- \textipa{\*ri@s}. But, when we consider the segmentation produced by the \texttt{phonemizer}, ``various'' has one phoneme \textipa{i@} and ``serious'' has two phonemes --- \textipa{i} and \textipa{@}.

\begin{table}[]
	\centering
	\begin{tabular}{l|ll}
		Phonemes & \multicolumn{2}{c}{Examples} \\ \hline
		``\textipa{i}'', ``\textipa{@}'' and ``\textipa{i@}''& \textipa{s}$\mid$\textipa{I}$\mid$\textipa{\*r}$\mid$\textbf{\textipa{i}$\mid$\textipa{@}}$\mid$\textipa{s} & \textipa{v}$\mid$\textipa{E}$\mid$\textipa{\*r}$\mid$\textbf{\textipa{i@}}$\mid$\textipa{s} \\
		``\textipa{aI}'', ``\textipa{@}'' and ``\textipa{aI@}'' & \textipa{dZ}$\mid$\textbf{\textipa{aI}$\mid$\textipa{@}}$\mid$\textipa{n}$\mid$\textipa{t}  & \textipa{k}$\mid$\textipa{w}$\mid$\textbf{\textipa{aI@}}$\mid$\textipa{t}  
	\end{tabular}
	\caption[Examples of multi-character phonemes]{Examples of multi-character phonemes consisting of other single- and multi-character phonemes. ``$\mid$'' is the segmentation mark. These segmentation marks are present in ASR training data, but not in the language model training data.}
	\label{tab:phon_examples}
\end{table}

To test this assumption, we train a 3-gram language models (for single- and multi-character ASR) on ASR training data (notably a considerably smaller amount of data then CzEng) that include phoneme segmentation (e.g., ``\textipa{v}$\mid$\textipa{E}$\mid$\textipa{\*r}$\mid$\textbf{\textipa{i@}}$\mid$\textipa{s}''). Note, we do not re-phonemize CzEng, as phonemization is time and power-consuming process. Using these data that include segmentation, we correctly substitute multi-character phonemes. After applying the substitution of the multi-character phonemes, we remove the phoneme separators (as the CTC decoder's queries to the LM are without separators).

The best results are 8.30 and 7.74 \% PWER for multi- and single-character ASR respectively. These results are consistent with the greedy and beam search without LM decoding. Therefore, we conclude that using native phoneme segmentation is slightly better. Hence, for English, we use multi-character phonemes.


\begin{figure*}[t]
	\includegraphics[width=\linewidth,height=7cm]{img/cs_phon}
	\caption[Learning curves for Czech acoustic model]{Learning curves on phonemized Czech Parliament Hearings development set.}
	\label{fig:cs_phon}
\end{figure*}

\begin{table}[t]
	\centering
	\begin{tabular}{lc|c|ccc}
		\multirow{2}{*}{Type} & \multirow{2}{*}{Method} & Adaptation & \multicolumn{3}{c}{Full training} \\
		&                       & phase & greedy & beam s. & beam s. with LM \\ \hline
		\multirow{2}{*}{dev}  & multi-char. phonemes  & 13.75 & 7.09   & -       & -               \\
		& single-char. phonemes & 17.91 & 10.04  & -       & -               \\ \hline
		\multirow{2}{*}{test} & multi-char. phonemes  & -     & 8.94   & 9.14    & 8.30            \\
		& single-char. phonemes & -     & 9.09   & 9.53    & 7.84           
	\end{tabular}
	\caption[Results of Czech acoustic models]{Results in \% of \emph{Phoneme} Word Error Rate (PWER). The language model is trained on CzEng. Note, PWER is not directly comparable to WER. The column ``Adaptation phase'' represents model performance after the adaptation of the decoder to new modeling units but before further training (i.e., the encoder is frozen and only decoder is trained).}
	\label{tab:cs_phon_results}
\end{table}

\paragraph{English}
The course of the English training was carried out on two development sets and it is pictured in \cref{fig:en_phon}. The final results on the development and test sets are in \cref{tab:en_phon_results}. 

We again test beam search with and without a language model. We train a 3-gram LM and use ASR training data, as these contain separated phonemes. The obtained results could be, therefore, better if we took more training data for the language model and bigger $n$-grams.

Again, as the observed performance in \cref{tab:en_phon_results} suggests, beam search decoding performs worse than greedy decoding. Adding a language model to re-score beams helps to reduce PWER. There is a slight reduction in the Libri Speech test-clean. Test-other gets better about 3 \% PWER absolute. For the Common Voice test set, the PWER reduction is considerable --- almost 40 \% relative (6.46 vs. 10.21) compared to the greedy decoding. As a possible explanation why LM helps Common Voice so much is that nearly two-thirds of the LM training data come from this data set and there is a significant overlap of sentences in the training data and the test set. 

\begin{figure*}[t]
	\includegraphics[width=\linewidth,height=7cm]{img/en_phon}
	\caption[Learning curves for English acoustic model]{Learning curves on phonemized Libri Speech \texttt{dev-clean} and Common Voice \texttt{dev}.}
	\label{fig:en_phon}
\end{figure*}

\begin{table}[t]
	\centering
	\begin{tabular}{lc|c|ccc}
		\multirow{2}{*}{Type} & \multirow{2}{*}{Corpus} & Adaptation & \multicolumn{3}{c}{Full training} \\
		&                    &   phase    & greedy & beam s. & beam s. with LM \\ \hline
		\multirow{2}{*}{dev}   & Libri Speech Clean & 46.07 & 3.84   &      -       &           -          \\
		& Common Voice       & 54.69 & 11.86  &       -      &         -            \\ \hline
		\multirow{3}{*}{test} & Libri Speech Clean & -     & 4.18   & 4.48        & 3.58                \\
		& Libri Speech Other & -     & 11.48  & 11.67       & 8.57                \\
		& Common Voice       & -     & 10.21  & 10.47       & 6.46               
	\end{tabular}
	\caption[Results of English acoustic models]{Results in \% of \emph{Phoneme} Word Error Rate (PWER). The language model is trained on phonemized ASR training data. Note, PWER is not directly comparable to WER. \XXX{The column ``Adaptation phase'' represents model performance after the adaptation of the decoder to new modeling units but before further training (i.e., the encoder is frozen and only decoder is trained).}}
	\label{tab:en_phon_results}
\end{table}


\subsection{Conclusion and Future Work}
We proposed an alternative text representation for ASR --- phonemes --- and trained models for Czech and English language. Further, we examined two different approaches for phoneme encoding: respecting multi-character phonemes and single-character ones. We concluded that using native, multi-character phonemes is slightly better. We also successfully trained language models for use with phonetic transcriptions. 

The question whether the phoneme-based ASR works better than grapheme-based ASR, cannot be answered simply. The mapping between graphemes and phonemes is not straightforward. Therefore, we leave it to the next chapter.

In the future, we would like to examine coarse-to-fine methods for improving phoneme-based ASR. We believe that similarly to grapheme-based ASR as observed in \cref{asr:crosslingual_intermediate}, this could reduce the training time and lower the final PWER. 

Another technique, multi-task learning, proposed and applied in HMM-based ASR by \perscite{chen2014joint}, could bring a further performance boost. 


